{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN using CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Convolutional Generative Adversarial Network [DCGAN] is a Unsupervised Learning Technique that learns a hierarchy of representations from object parts to scenes in both the Generator and Discriminator. The Generative Adversarial Networks [GAN] was introduced by Ian Goodfellow, an architecture whose aim was to learn from the errors or the loss of the output that the generator generated and tune it's parameters such that it is able to generate the output that fools a discriminator. A discriminator is nothing but a classifier that trains on the data and learns to classify the data generated by the discriminator as fake or real.\n",
    "\n",
    "In this tutorial, we'll be implementing the DCGAN Architecture, slightly modified to fit for the CIFAR-10 Dataset and see how the DCGAN is able to generate CIFAR-10 images from noise input by learning the representations and decreasing the generator loss.\n",
    "\n",
    "So, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import numpy as np\n",
    "\n",
    "# Dataset\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Common Layers\n",
    "from keras.layers import Dense, Activation, BatchNormalization, Reshape, Flatten \n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Layers specific to Generator\n",
    "from keras.layers import Conv2DTranspose\n",
    "\n",
    "# Layers specific to Discriminator\n",
    "from keras.layers import Conv2D, LeakyReLU \n",
    "\n",
    "# Use this to pass an element-wise TensorFlow/Theano/CNTK function as an activation\n",
    "import keras.backend as k\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this code, I'll be using the CIFAR-10 Dataset.\n",
    "\n",
    "The CIFAR-10 dataset will be laoded using the Keras \"load_data\" functionality. When we load the data using this, it is loaded into training and test set as a tuple of each. i.e. a tuple of training features and labels and a tuple of test features and labels.\n",
    "\n",
    "Then we will analyze the dataset for its number of features and labels and visualize the dataset at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: Load and Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data Analysis\n",
    "print('Training Data: \\n')\n",
    "print('Num. Features: ',len(X_train)), print('Num. Labels: ',len(y_train))\n",
    "print('Shape of Features: ',X_train.shape), print('Shape of Labels: ',y_train.shape)\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Test Data: \\n')\n",
    "print('Num. Features: ',len(X_test)), print('Num. Labels: ',len(y_test))\n",
    "print('Shape of Features: ',X_test.shape), print('Shape of Labels: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we can see, there are 50,000 training features and labels and 10,000 test features and labels. The shape of the total dataset is :\n",
    "\n",
    "Training: (50000, 32, 32, 3)\n",
    "\n",
    "Test: (10000, 32, 32, 3)\n",
    "\n",
    "Let's take a look at the individual image shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of One Image\n",
    "rand_idx = np.random.randint(0, len(X_train), 1)\n",
    "print('Shape of one Image: ', X_train[rand_idx].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image in the CIFAR-10 dataset is of the shape (32, 32, 3) i.e. a total of 3072 pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Images\n",
    "label_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(10,5))\n",
    "for i in range(0,3):\n",
    "    for j in range(0,3):\n",
    "        idx = np.random.randint(0, len(X_train), 1)\n",
    "        idx = idx[0]\n",
    "        ax[i,j].imshow(X_train[idx])\n",
    "        ax[i,j].set_axis_off()\n",
    "        ax[i,j].title.set_text('Label: {}'.format(label_names[y_train[idx][0]]))\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the paper, the input images were scaled to the range of [-1,1]. Using this ensures that each input parameter i.e. the pixels in the case of images have a similar data distribution. This helps as it speeds up the convergence while training the model. This also helps to avoid vanishing gradient problem while backpropagation. \n",
    "\n",
    "So, next, we'll write a function that does just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a random image and looking at its pixel values\n",
    "idx = np.random.randint(0, len(X_train), 1)\n",
    "print('Image Index No.: ', idx)\n",
    "print('\\nImage Pixel Values [Before Normalization]: \\n\\n',X_train[idx])\n",
    "print('\\n\\n Shape of Image: ',X_train[idx].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above lines, we take a random image and see that it has the values that lie in the range (0,255). So, let's write a function to take in these images and normalize them to the range of (-1,1). Also, the paper tells that the dense layer i.e. the layer before the first convolution layer for the generator is a 4-D tensor. So, we'll reshape all of the training data here as well and add a 4th dimension up front.\n",
    "\n",
    "Also, we'll also write a de-normalizing function to reshape the generated images to the shape (28,28) and the pixel values in the range (0,255) before plotting the final generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply Normalization similar to tanh activation function range i.e. [-1,1]\n",
    "def normalize_images(img):\n",
    "    img = img.reshape(-1,32,32,3)\n",
    "    img = np.float32(img)\n",
    "    img = (img / 255 - 0.5) * 2\n",
    "    img = np.clip(img, -1, 1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to DeNormalize the Images once we are done Training the DCGAN Model\n",
    "def denormalize_images(img):\n",
    "    img = (img / 2 + 1) * 255\n",
    "    img = np.clip(img, 0, 255)\n",
    "    img = np.uint8(img)\n",
    "    img = img.reshape(32, 32, 3)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we are done with the normalizing and denormalizing functions, let's normalize the training and test features and have a look at the same image after normalization and the overall shape of the train and test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the Training and Test Features\n",
    "X_train = normalize_images(X_train)\n",
    "X_test = normalize_images(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Normalization Function\n",
    "print('Image Index No.: ', idx)\n",
    "print('\\nImage Pixel Values [After Normalization]: \\n\\n',X_train[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we can see, using normalization our image pixel values have been changed from the scale of (0,255) to the scale of (-1,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: DCGAN Generator Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper describes the DCGAN Generator Architecture as shown in the following image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Generator Architecture\n",
    "from IPython.display import Image\n",
    "Image(filename='./Images/generator.png', width=900) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Generator for DCGAN has the following components:\n",
    "\n",
    "**1. Input Layer [Dense]:**\n",
    "\n",
    "This layer is where we provide the noise Input using which, over time and training the Generator is able to convert into an image at the output.\n",
    "\n",
    "This layer is a Fully Connected or Dense layer that takes the pixels of the image. For this tutorial, we'll be using the input units with a dimension of 7*7*128. As per the paper, the input shape for the dense layer is 100z, so, we'l be using the input shape as 100.\n",
    "\n",
    "**2. Reshape:**\n",
    "\n",
    "Before giving the data into the Transposed Convolution function, we need to reshape the input data so that we can convert the array of values to a matrix and apply convolution operations on it.\n",
    "\n",
    "Input Shape: **[7*7*128]**\n",
    "\n",
    "After Reshaping, Input to Transpose Convolution Function: **[7,7,128]**\n",
    "\n",
    "**3. 2-D Transposed Convolution [Conv2DTranspose]:**\n",
    "\n",
    "As per the architecture of DCGAN mentioned in the paper, the Generator performs a series of Transposed Convolutions after getting the data from the dense layer and at the final layer we get a 64x64 image from these high level representations.\n",
    "Since, the first Transposed Convolution layer has a shape of 7 x 7 x 128 with a filter of 5 x 5 and stride of 2, the next transposed convolution layer will have the shape of 14 x 14 x 64, and then finally 28 x 28 x 1 i.e. the image.\n",
    "\n",
    "**4. Activation Functions [ReLU, Tanh]:**\n",
    "\n",
    "As per the paper, the Transposed Convolution layers use the ReLU activation function whereas we use a tanh activation function for the final layer. Using the bounded activation function allows the model to learn more quickly to saturate and cover the color space of the training distribution.\n",
    "\n",
    "** -------------------------------------------------------------------------------------------------------------------------- **\n",
    "\n",
    "**NOTE:** In case you are thinking that from where all these shapes for the convolutions appeared from, let's have a look.\n",
    "1. We know that, each MNIST image has a shape of  28 x 28 x 1 where 1 is the color channel and since the image is black and white, so it's 1.\n",
    "2. Look at the generator from the image side. You have an image with the shape 28x28x1 as input.\n",
    "3. Then, for filter of 5x5, stride of 2 and same padding, we can find out the shape of the next output as follows:\n",
    "\n",
    "### output shape = (W + 2*P - F) + 1 / S\n",
    "\n",
    "where, \n",
    "\n",
    "**W:** Width of Input Image\n",
    "\n",
    "**P:** Padding\n",
    "\n",
    "**F:** Size of Filter\n",
    "\n",
    "**S:** Stride\n",
    "\n",
    "So, using the values that we defined above, we get the output as:\n",
    "\n",
    "** Shape of output after 1st convolution:** (28 + 2*2 - 5) + 1 / 2  => 14 or  14 x 14 x number of filters\n",
    "\n",
    "So, if the number of filters = 64, the shape of output becomes:  14 x 14 x 64\n",
    "\n",
    "Similarly, for the next layer with the same configuration, the shape will be: 7 x 7 x 128.\n",
    "\n",
    "Well, you might think that why I said convolutions and not Transposed Convolutions, that is because we went from image to last filter. This is what is the architecture for Discriminator. If we just use this in the opposite order, it becomes the size for the Generator. You will see more when we print out the summary of the Generator and the Discriminator.\n",
    "\n",
    "** -------------------------------------------------------------------------------------------------------------------------- **\n",
    "\n",
    "So, let's define the Generator Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "def generator(inputSize):\n",
    "    generator_model = Sequential()\n",
    "    # Input Dense Layer\n",
    "    generator_model.add(Dense(4*4*256, input_shape=(inputSize,)))\n",
    "    # Reshape the Input, apply Batch Normalization and Leaky ReLU Activation.\n",
    "    generator_model.add(Reshape(target_shape=(4,4,256)))\n",
    "    generator_model.add(BatchNormalization())\n",
    "    generator_model.add(Activation('relu'))\n",
    "    \n",
    "    # First Transpose Convolution Layer\n",
    "    generator_model.add(Conv2DTranspose(filters=128, kernel_size=5, strides=2, padding='same'))\n",
    "    generator_model.add(BatchNormalization())\n",
    "    generator_model.add(Activation('relu'))\n",
    "        \n",
    "    # Since, we are using MNIST Data which has only 1 channel, so filter for Generated Image = 1\n",
    "    generator_model.add(Conv2DTranspose(filters=64, kernel_size=5, strides=2, padding='same'))\n",
    "    generator_model.add(BatchNormalization())\n",
    "    generator_model.add(Activation('relu'))\n",
    "    \n",
    "    generator_model.add(Conv2DTranspose(filters=3, kernel_size=5, strides=2, padding='same'))\n",
    "    generator_model.add(Activation('tanh'))\n",
    "    \n",
    "    generator_model.summary()\n",
    "    \n",
    "    return generator_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-5: DCGAN Discriminator Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper describes the DCGAN Discriminator Architecture as shown in the following image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Discriminator Architecture\n",
    "from IPython.display import Image\n",
    "Image(filename='./Images/discriminator.png', width=900) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator for DCGAN has the following components:\n",
    "\n",
    "**1. 2-D Convolution [Conv2D]:**\n",
    "\n",
    "Since, the aim of the discriminator is to classify images between real and fake, it takes in the complete image generated by the generator and try to tell that whether it is a true or a fake image. Hence, CNN comes into play as they are the state of the art networks for image classification. So, we use Convolution filters for the first 3 layers as opposed to Transpose Convolution in the Generator.\n",
    "\n",
    "\n",
    "**2. Activation Functions [LeakyReLU]:**\n",
    "\n",
    "As per the paper, the Convolution layers use the LeakyReLU activation function throughout the discriminator layers. Using the bounded activation function allows the model to learn more quickly to saturate and cover the color space of the training distribution.\n",
    "\n",
    "**NOTE:** The shapes for the Discriminator have been defined in the Generator text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "def discriminator(leakSlope):\n",
    "    discriminator_model = Sequential()\n",
    "    \n",
    "    # Input and First Conv2D Layer\n",
    "    discriminator_model.add(Conv2D(filters=64, kernel_size=5, strides=2, padding='same', input_shape=(28,28,1)))\n",
    "    discriminator_model.add(LeakyReLU(alpha=leakSlope))\n",
    "    \n",
    "    # Second Conv2D Layer\n",
    "    discriminator_model.add(Conv2D(filters=128, kernel_size=5, strides=2, padding='same'))\n",
    "    discriminator_model.add(BatchNormalization())\n",
    "    discriminator_model.add(LeakyReLU(alpha=leakSlope))\n",
    "    \n",
    "    discriminator_model.add(Conv2D(filters=256, kernel_size=5, strides=2, padding='same'))\n",
    "    discriminator_model.add(BatchNormalization())\n",
    "    discriminator_model.add(LeakyReLU(alpha=leakSlope))\n",
    "    \n",
    "    discriminator_model.add(Flatten())\n",
    "    discriminator_model.add(Dense(32*32*3))\n",
    "    discriminator_model.add(BatchNormalization())\n",
    "    discriminator_model.add(LeakyReLU(alpha=leakSlope))\n",
    "    \n",
    "    # Output Layer\n",
    "    discriminator_model.add(Dense(3))\n",
    "    discriminator_model.add(Activation('sigmoid'))\n",
    "    \n",
    "    discriminator_model.summary()\n",
    "    \n",
    "    return discriminator_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the Generator and the Discriminator, we just need to combine these two in a single place to form a DCGAN architecture. So, let's do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-6: DCGAN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper defines the DCGAN Architecture as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Discriminator Architecture\n",
    "from IPython.display import Image\n",
    "Image(filename='./Images/complete_dcgan.png', width=900) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DCGAN Architecture\n",
    "def DCGAN(sample_size, generator_lr, generator_momentum, discriminator_lr, discriminator_momentum, leakyAlpha, show_summary=False):\n",
    "    \n",
    "    # Clear Session\n",
    "    k.clear_session()\n",
    "    \n",
    "    # Generator\n",
    "    gen = generator(inputSize=100)\n",
    "    \n",
    "    # Discrimintor\n",
    "    dis = discriminator(leakSlope=0.2)\n",
    "    dis.compile(loss='binary_crossentropy', optimizer=Adam(lr=discriminator_lr, beta_1=discriminator_momentum))\n",
    "    \n",
    "    dis.trainable = False\n",
    "    \n",
    "    dcgan = Sequential([gen, dis])\n",
    "    dcgan.compile(loss='binary_crossentropy', optimizer=Adam(lr=generator_lr, beta_1=generator_momentum))\n",
    "    \n",
    "    if show_summary == True:\n",
    "        print(\"\\n Generator Model Summary: \\n\")\n",
    "        gen.summary()\n",
    "        \n",
    "        print(\"\\n\\n Discriminator Model Summary: \\n\")\n",
    "        dis.summary()\n",
    "        \n",
    "        print(\"\\n\\nDCGAN Model Summary\\n\")\n",
    "        dcgan.summary()\n",
    "    \n",
    "    return dcgan, gen, dis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above block, we define the DCGAN Architecture where we combine the Generator and the Discriminator into one. We'll be using Adam Optimizer for the training of the generator and the discriminator with a binary_crossentropy loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function to Plot Images after every 20 Epochs of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Plot Images\n",
    "def plot_images(generated_images):\n",
    "    n_images = len(generated_images)\n",
    "    rows = 4\n",
    "    cols = n_images//rows\n",
    "    \n",
    "    plt.figure(figsize=(cols, rows))\n",
    "    for i in range(n_images):\n",
    "        img = denormalize_images(generated_images[i])\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model ready, it's time to train our model. In the function below, we will generate a random noise data which will be passed into the generator as input. Over the time, the generator will learn from it's loss the correct features and eventually start outputting the MNIST images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-7: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Train the Model\n",
    "def train_model(sample_size, generator_lr, generator_momentum, discriminator_lr, discriminator_momentum, leakyAlpha, epochs, batch_size, eval_size, smooth):\n",
    "    \n",
    "    # To Do: Add Label Noise Data\n",
    "    # Training Labels [Real, Fake]\n",
    "    training_labels = [np.ones([batch_size, 1]), np.zeros([batch_size, 1])]\n",
    "    \n",
    "    # Test Labels [Real, Fake]\n",
    "    test_labels = [np.ones([eval_size, 1]), np.zeros([eval_size, 1])]\n",
    "    \n",
    "    # Total Number of Batches = (Total Training Images / Images per Batch)\n",
    "    num_batches = (len(X_train) // batch_size)\n",
    "    \n",
    "    # Call the DCGAN Architecture\n",
    "    dcgan, generator, discriminator = DCGAN(sample_size, generator_lr, generator_momentum, discriminator_lr, discriminator_momentum, leakyAlpha, show_summary=True)\n",
    "    \n",
    "    # Array to Store Cost/Loss Values\n",
    "    cost = []\n",
    "    \n",
    "    # Train the Generator and Discriminator\n",
    "    for i in range(epochs):\n",
    "        for j in range(num_batches):\n",
    "            \n",
    "            # Noise Input for Generator\n",
    "            # Mean = 0, Stddev = 1\n",
    "            noise_data = np.random.normal(loc=0, scale=1, size=(batch_size, sample_size))\n",
    "            \n",
    "            # Make Predictions using Generator and Generate Fake Images\n",
    "            fake_images = generator.predict_on_batch(noise_data)\n",
    "            \n",
    "            # Load MNIST Data in Batches\n",
    "            # [0:128], [128:256], ...\n",
    "            train_image_batch = X_train[j*batch_size:(j+1)*batch_size]\n",
    "            \n",
    "            # Train the Discriminator\n",
    "            discriminator.trainable = True\n",
    "            \n",
    "            # Train the Discriminator on Training Data and Labels\n",
    "            discriminator.train_on_batch(train_image_batch, training_labels[0] * (1 - smooth))\n",
    "            \n",
    "            # Train Discriminator on Fake Generated Images and Labels\n",
    "            discriminator.train_on_batch(fake_images, training_labels[1])\n",
    "            \n",
    "            # Set Discriminator training to False when Generator is Training\n",
    "            discriminator.trainable = False\n",
    "            \n",
    "            # Train the Generator on Noise Data Input with Training Labels to reduce Cost/Loss\n",
    "            # This way, the Discriminator gets trained twice for each one training step of Generator\n",
    "            dcgan.train_on_batch(noise_data, training_labels[0])\n",
    "        \n",
    "            \n",
    "        # To Do: Add Eval Code\n",
    "        # Eval/Test Features [Real,Fake]\n",
    "        real_eval_features = X_test[np.random.choice(len(X_test), size= eval_size, replace=False)]\n",
    "        \n",
    "        # Eval Noise Data\n",
    "        noise_data = np.random.normal(loc=0, scale=1, size=(eval_size, sample_size)) \n",
    "        \n",
    "        # Fake Eval Features: Creates the Images to Fool the Discriminator\n",
    "        fake_eval_features = generator.predict_on_batch(noise_data)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        # Discriminator Loss: Actual Training Loss for Classification + Loss on Fake Data\n",
    "        discriminator_loss  = discriminator.test_on_batch(real_eval_features, test_labels[0])\n",
    "        discriminator_loss += discriminator.test_on_batch(fake_eval_features, test_labels[1])\n",
    "        \n",
    "        # Generator Loss: DCGAN Loss\n",
    "        generator_loss  = dcgan.test_on_batch(noise_data, test_labels[0])\n",
    "        \n",
    "        # Add calculated cost/loss to array for plotting\n",
    "        cost.append((discriminator_loss, generator_loss))\n",
    "        \n",
    "        print(\"Epochs: {0}, Generator Loss: {1}, Discriminator Loss: {2}\".format(i+1, generator_loss, discriminator_loss))\n",
    "       \n",
    "        # Plot the Images and Save them after every 10 epochs\n",
    "        if ((i+1)%10 == 0):\n",
    "            plot_images(fake_eval_features)\n",
    "        \n",
    "    # Save Trained Models\n",
    "    generator.save('./cifar_generator.h5')\n",
    "    discriminator.save('./cifar_discriminator.h5')\n",
    "    dcgan.save('./cifar_dcgan.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_size, generator_lr, generator_momentum, discriminator_lr, discriminator_momentum, leakyAlpha, epochs, batch_size, eval_size, smooth\n",
    "train_model(sample_size=100,generator_lr=0.0001, generator_momentum=0.9, discriminator_lr=0.001, discriminator_momentum=0.9, leakyAlpha=0.2, epochs=100, batch_size=128, eval_size=16, smooth=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, finally after 100 iterations we see that the DCGAN Model has learnt to generate the CIFAR-10 images using generator and fool the discriminator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN using MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import numpy as np\n",
    "\n",
    "# Dataset\n",
    "from keras.datasets.mnist import load_data\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Common Layers\n",
    "from keras.layers import Dense, Activation, BatchNormalization, Reshape, Flatten \n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Layers specific to Generator\n",
    "from keras.layers import Conv2DTranspose\n",
    "\n",
    "# Layers specific to Discriminator\n",
    "from keras.layers import Conv2D, LeakyReLU \n",
    "\n",
    "# Use this to pass an element-wise TensorFlow/Theano/CNTK function as an activation\n",
    "import keras.backend as k\n",
    "\n",
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this code, I'll be starting with the MNIST Dataset and then using the same architecture on other datasets like CIFAR, Faces Dataset etc.\n",
    "\n",
    "The MNIST dataset will be laoded using the Keras \"load_data\" functionality. When we load the data using this, it is loaded into training and test set as a tuple of each. i.e. a tuple of training features and labels and a tuple of test features and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset and Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "(X_train, y_train), (X_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data Analysis\n",
    "print('Training Data: \\n')\n",
    "print('Num. Features: ',len(X_train)), print('Num. Labels: ',len(y_train))\n",
    "print('Shape of Features: ',X_train.shape), print('Shape of Labels: ',y_train.shape)\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Test Data: \\n')\n",
    "print('Num. Features: ',len(X_test)), print('Num. Labels: ',len(y_test))\n",
    "print('Shape of Features: ',X_test.shape), print('Shape of Labels: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of One Image\n",
    "rand_idx = np.random.randint(0, len(X_train), 1)\n",
    "print('Shape of one Image: ', X_train[rand_idx].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Images\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(10,5))\n",
    "for i in range(0,3):\n",
    "    for j in range(0,3):\n",
    "        idx = np.random.randint(0, len(X_train), 1)\n",
    "        idx = idx[0]\n",
    "        ax[i,j].imshow(X_train[idx], cmap='gray')\n",
    "        ax[i,j].set_axis_off()\n",
    "        ax[i,j].title.set_text('Label: {}'.format(y_train[idx]))\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the paper, the input images were scaled to the range of the tanh activation function i.e. [-1,1]. Using this ensures that each input parameter i.e. the pixels in the case of images have a similar data distribution. This helps as it speeds up the convergence while training the model. So, next, we'll write a function that does just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a random image and looking at its pixel values\n",
    "idx = np.random.randint(0, len(X_train), 1)\n",
    "print('Image Index No.: ', idx)\n",
    "print('\\nImage Pixel Values [Before Normalization]: \\n\\n',X_train[idx])\n",
    "print('\\n\\n Shape of Image: ',X_train[idx].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply Normalization similar to tanh activation function range i.e. [-1,1]\n",
    "def normalize_images(img):\n",
    "    # Reshape Image from 28,28,1 to -1,28,28,1\n",
    "    img = img.reshape(-1,28,28,1)\n",
    "    img = np.float32(img)\n",
    "    img = (img / 255 - 0.5) * 2\n",
    "    img = np.clip(img, -1,1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Function\n",
    "img = normalize_images(X_train[3337])\n",
    "print('Normalized Pixel Values: \\n\\n', img)\n",
    "print('\\n\\n Shape of Normalized Image: ', img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the Training and Test Features\n",
    "X_train = normalize_images(X_train)\n",
    "X_test = normalize_images(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN Generator Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper describes the DCGAN Architecture as shown in the following image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Generator Architecture\n",
    "from IPython.display import Image\n",
    "Image(filename='./Images/generator.png', width=900) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Generator for DCGAN has the following components:\n",
    "\n",
    "**1. Input Layer [Dense]:**\n",
    "\n",
    "This layer is where we provide the noise Input using which, over time and training the Generator is able to convert into an image at the output.\n",
    "\n",
    "This layer is a Fully Connected or Dense layer that takes the pixels of the image in the current case. Since, we know that the MNIST Image has a shape of [28,28,1], so the total number of Input values is 28*28 = 784.\n",
    "\n",
    "**2. Reshape:**\n",
    "\n",
    "Before giving the data into the Transposed Convolution function, we need to resize the input data so that we can apply convolution operation on it.\n",
    "\n",
    "Input Shape: **[1,784]**\n",
    "\n",
    "After Reshaping, Input to Transpose Convolution Function: **[4,4,1024]**\n",
    "\n",
    "**3. 2-D Transposed Convolution [Conv2DTranspose]:**\n",
    "\n",
    "As per the architecture of DCGAN mentioned in the paper, the Generator performs a series of Transposed Convolutions after getting the data from the dense layer and at the final layer we get a 64x64 image from these high level representations.\n",
    "\n",
    "**4. Activation Functions [ReLU, Tanh]:**\n",
    "\n",
    "As per the paper, the Transposed Convolution layers use the ReLU activation function whereas we use a tanh activation function for the final layer. Using the bounded activation function allows the model to learn more quickly to saturate and cover the color space of the training distribution.\n",
    "\n",
    "So, let's define the Generator Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "def generator(inputSize, leakSlope):\n",
    "    generator_model = Sequential()\n",
    "    # Input Dense Layer\n",
    "    generator_model.add(Dense(784, input_shape=(inputSize,)))\n",
    "    # Reshape the Input, apply Batch Normalization and Leaky ReLU Activation.\n",
    "    generator_model.add(Reshape(target_shape=(7,7,16)))\n",
    "    generator_model.add(BatchNormalization())\n",
    "    generator_model.add(Activation('relu'))\n",
    "    \n",
    "    # First Transpose Convolution Layer\n",
    "    generator_model.add(Conv2DTranspose(filters=32, kernel_size=5, strides=2, padding='same'))\n",
    "    generator_model.add(BatchNormalization())\n",
    "    generator_model.add(Activation('relu'))\n",
    "    \n",
    "    # Second Transpose Convolution Layer\n",
    "    # generator_model.add(Conv2DTranspose(filters=256, kernel_size=5, strides=2, padding='same'))\n",
    "    # generator_model.add(BatchNormalization())\n",
    "    # generator_model.add(Activation('relu'))\n",
    "    \n",
    "    # Third Transpose Convolution Layer\n",
    "    # generator_model.add(Conv2DTranspose(filters=128, kernel_size=5, strides=2, padding='same'))\n",
    "    # generator_model.add(BatchNormalization())\n",
    "    # generator_model.add(Activation('relu'))\n",
    "    \n",
    "    # Since, we are using MNIST Data which has only 1 channel, so filter for Generated Image = 1\n",
    "    generator_model.add(Conv2DTranspose(filters=1, kernel_size=5, strides=2, padding='same'))\n",
    "    generator_model.add(Activation('tanh'))\n",
    "    \n",
    "    # Print Model Summary\n",
    "    generator_model.summary()\n",
    "    \n",
    "    return generator_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN Discriminator Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper describes the DCGAN Architecture as shown in the following image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Discriminator Architecture\n",
    "from IPython.display import Image\n",
    "Image(filename='./Images/discriminator.png', width=900) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator for DCGAN has the following components:\n",
    "\n",
    "**1. 2-D Convolution [Conv2D]:**\n",
    "\n",
    "Since, the aim of the discriminator is to classify images between real and fake, it takes in the complete image generated by the generator and try to tell that whether it is a true or a fake image. Hence, CNN comes into play as they are the state of the art networks for image classification. So, we use Convolution filters for the first 3 layers as opposed to Transpose Convolution in the Generator.\n",
    "\n",
    "\n",
    "**2. Activation Functions [LeakyReLU]:**\n",
    "\n",
    "As per the paper, the Convolution layers use the LeakyReLU activation function throughout the discriminator layers. Using the bounded activation function allows the model to learn more quickly to saturate and cover the color space of the training distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "def discriminator(leakSlope):\n",
    "    discriminator_model = Sequential()\n",
    "    \n",
    "    # Input and First Conv2D Layer\n",
    "    discriminator_model.add(Conv2D(filters=32, kernel_size=5, strides=2, padding='same', input_shape=(28,28,1)))\n",
    "    discriminator_model.add(LeakyReLU(alpha=leakSlope))\n",
    "    \n",
    "    # Second Conv2D Layer\n",
    "    discriminator_model.add(Conv2D(filters=16, kernel_size=5, strides=2, padding='same'))\n",
    "    discriminator_model.add(BatchNormalization())\n",
    "    discriminator_model.add(LeakyReLU(alpha=leakSlope))\n",
    "    \n",
    "    # Third Conv2D Layer\n",
    "    # discriminator_model.add(Conv2D(filters=256, kernel_size=5, strides=2, padding='same'))\n",
    "    # discriminator_model.add(BatchNormalization())\n",
    "    # discriminator_model.add(LeakyReLU(alpha=leakSlope))\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    discriminator_model.add(Flatten())\n",
    "    discriminator_model.add(Dense(784))\n",
    "    discriminator_model.add(BatchNormalization())\n",
    "    discriminator_model.add(LeakyReLU(alpha=leakSlope))\n",
    "    \n",
    "    # Output Layer\n",
    "    discriminator_model.add(Dense(1))\n",
    "    discriminator_model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # Model Summary\n",
    "    discriminator_model.summary()\n",
    "    \n",
    "    return discriminator_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now since that we have defined the Generator and the Discriminator for the DCGAN Architecture, let's bring them together and complete the DCGAN Architecture.\n",
    "\n",
    "The complete DCGAN Architecture looks like as image below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete DCGAN Architecture\n",
    "from IPython.display import Image\n",
    "Image(filename='./Images/complete_dcgan.png', width=900) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN Architecture\n",
    "def DCGAN(sample_size, generator_lr, generator_momentum_beta, discriminator_lr, discriminator_momentum_beta, leakyAlpha):\n",
    "    # Generator\n",
    "    gen = generator(inputSize=sample_size, leakSlope=leakyAlpha)\n",
    "    \n",
    "    # Discriminator\n",
    "    disc = discriminator(leakSlope=leakyAlpha)\n",
    "    \n",
    "    # Discriminator Optimizer\n",
    "    disc.compile(optimizer=Adam(lr=discriminator_lr, beta_1=discriminator_momentum_beta), loss='binary_crossentropy')\n",
    "    \n",
    "    # DCGAN\n",
    "    dcgan = Sequential()\n",
    "    dcgan.add(gen)\n",
    "    disc.trainable = False\n",
    "    dcgan.add(disc)\n",
    "    dcgan.compile(optimizer=Adam(lr=generator_lr, beta_1=generator_momentum_beta), loss='binary_crossentropy')\n",
    "    \n",
    "    return dcgan, gen, disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization with Zero Mean and Unit Variance\n",
    "def latent_samples(num_samples, sample_size):\n",
    "    return np.random.normal(loc=0, scale=1, size=(num_samples, sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- TEST --------------------------------\n",
    "def show_results(losses):\n",
    "    labels = ['Classifier', 'Discriminator', 'Generator']\n",
    "    losses = np.array(losses)    \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(losses.T[0], label='Discriminator')\n",
    "    plt.plot(losses.T[1], label='Generator')\n",
    "    plt.title(\"Training Losses\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------- TEST --------------------------------------------\n",
    "def show_images(generated_images):\n",
    "    n_images = len(generated_images)\n",
    "    rows = 4\n",
    "    cols = n_images//rows\n",
    "    \n",
    "    plt.figure(figsize=(cols, rows))\n",
    "    for i in range(n_images):\n",
    "        img = deprocess(generated_images[i])\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------ TEST --------------------------------------------\n",
    "def make_labels(size):\n",
    "    return np.ones([size, 1]), np.zeros([size, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------- TEST ------------------------------------------\n",
    "def train(\n",
    "    g_learning_rate,   # learning rate for the generator\n",
    "    g_beta_1,          # the exponential decay rate for the 1st moment estimates in Adam optimizer\n",
    "    d_learning_rate,   # learning rate for the discriminator\n",
    "    d_beta_1,          # the exponential decay rate for the 1st moment estimates in Adam optimizer\n",
    "    leaky_alpha,\n",
    "    smooth=0.1,        # label smoothing\n",
    "    sample_size=100,   # latent sample size (i.e. 100 random numbers)\n",
    "    epochs=100,\n",
    "    batch_size=128,    # train batch size\n",
    "    eval_size=16,      # evaluate size\n",
    "    show_details=True):\n",
    "    \n",
    "    # labels for the batch size and the test size\n",
    "    y_train_real, y_train_fake = make_labels(batch_size)\n",
    "    y_eval_real,  y_eval_fake  = make_labels(eval_size)\n",
    "\n",
    "    # create a GAN, a generator and a discriminator\n",
    "    gan, generator, discriminator = DCGAN(\n",
    "        sample_size, \n",
    "        g_learning_rate, \n",
    "        g_beta_1,\n",
    "        d_learning_rate,\n",
    "        d_beta_1,\n",
    "        leaky_alpha)\n",
    "\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        for i in range(len(X_train)//batch_size):\n",
    "            # real MNIST digit images\n",
    "            X_batch_real = X_train[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "            # latent samples and the generated digit images\n",
    "            latent_sample = latent_samples(batch_size, sample_size)\n",
    "            X_batch_fake = generator.predict_on_batch(latent_sample)\n",
    "\n",
    "            # train the discriminator to detect real and fake images\n",
    "            #make_trainable(discriminator, True)\n",
    "            discriminator.trainable = True\n",
    "            discriminator.train_on_batch(X_batch_real, y_train_real * (1 - smooth))\n",
    "            discriminator.train_on_batch(X_batch_fake, y_train_fake)\n",
    "\n",
    "            # train the generator via GAN\n",
    "            #make_trainable(discriminator, False)\n",
    "            discriminator.trainable = False\n",
    "            gan.train_on_batch(latent_sample, y_train_real)\n",
    "\n",
    "        # evaluate\n",
    "        X_eval_real = X_test[np.random.choice(len(X_test), eval_size, replace=False)]\n",
    "\n",
    "        latent_sample = latent_samples(eval_size, sample_size)\n",
    "        X_eval_fake = generator.predict_on_batch(latent_sample)\n",
    "\n",
    "        d_loss  = discriminator.test_on_batch(X_eval_real, y_eval_real)\n",
    "        d_loss += discriminator.test_on_batch(X_eval_fake, y_eval_fake)\n",
    "        g_loss  = gan.test_on_batch(latent_sample, y_eval_real) # we want the fake to be realistic!\n",
    "\n",
    "        losses.append((d_loss, g_loss))\n",
    "\n",
    "        print(\"Epoch:{:>3}/{} Discriminator Loss:{:>7.4f} Generator Loss:{:>7.4f}\".format(e+1, epochs, d_loss, g_loss))\n",
    "        \n",
    "        if show_details and (e+1)%10==0:\n",
    "            show_images(X_eval_fake)\n",
    "    if show_details:\n",
    "        show_results(losses)\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(g_learning_rate=0.0001, g_beta_1=0.9, d_learning_rate=0.001, d_beta_1=0.9, leaky_alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
